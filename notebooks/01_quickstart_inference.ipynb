{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedGemma Quickstart Inference\n",
    "\n",
    "This notebook demonstrates how to run inference with MedGemma 4B for chest X-ray triage.\n",
    "\n",
    "**Requirements:**\n",
    "- GPU with 16GB+ VRAM (or use quantization for smaller GPUs)\n",
    "- `transformers>=4.50.0`\n",
    "- `torch>=2.1.0`\n",
    "\n",
    "**Time to complete:** ~10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "# !pip install -U transformers accelerate torch Pillow requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoProcessor, AutoModelForImageTextToText\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load MedGemma Model\n",
    "\n",
    "We use `google/medgemma-4b-it` - the instruction-tuned 4B parameter multimodal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"google/medgemma-4b-it\"\n",
    "\n",
    "# Method 1: Using pipeline (simpler)\n",
    "print(\"Loading MedGemma via pipeline...\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"image-text-to-text\",\n",
    "    model=MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    ")\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Sample Images\n",
    "\n",
    "We use CC0-licensed chest X-ray images for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_from_url(url: str) -> Image.Image:\n",
    "    \"\"\"Load an image from URL.\"\"\"\n",
    "    response = requests.get(url, headers={\"User-Agent\": \"MedGemmaDemo\"})\n",
    "    response.raise_for_status()\n",
    "    return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "\n",
    "\n",
    "# Sample CXR images (CC0 licensed from Wikimedia Commons)\n",
    "SAMPLE_IMAGES = [\n",
    "    {\n",
    "        \"name\": \"Normal CXR\",\n",
    "        \"url\": \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\",\n",
    "        \"expected\": \"Non-Urgent\",\n",
    "    },\n",
    "]\n",
    "\n",
    "# Load images\n",
    "images = []\n",
    "for sample in SAMPLE_IMAGES:\n",
    "    try:\n",
    "        img = load_image_from_url(sample[\"url\"])\n",
    "        images.append({\"name\": sample[\"name\"], \"image\": img, \"expected\": sample[\"expected\"]})\n",
    "        print(f\"✓ Loaded: {sample['name']} ({img.size})\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Failed to load {sample['name']}: {e}\")\n",
    "\n",
    "print(f\"\\nLoaded {len(images)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Triage Prompts\n",
    "\n",
    "We create structured prompts for:\n",
    "1. **Urgency Classification** - Urgent vs Non-Urgent\n",
    "2. **Brief Explanation** - One-line rationale\n",
    "3. **Key Findings** - Detailed observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an expert radiologist assistant. Your role is to help triage chest X-rays by:\n",
    "1. Classifying urgency (Urgent or Non-Urgent)\n",
    "2. Providing a brief explanation\n",
    "3. Highlighting key findings\n",
    "\n",
    "⚠️ IMPORTANT: This is for clinical decision support only. All findings require verification by a qualified radiologist.\"\"\"\n",
    "\n",
    "TRIAGE_PROMPT = \"\"\"Analyze this chest X-ray and provide:\n",
    "\n",
    "1. URGENCY: [Urgent/Non-Urgent]\n",
    "2. EXPLANATION: [One-line explanation for the urgency classification]\n",
    "3. KEY FINDINGS: [List 2-3 key observations]\n",
    "\n",
    "Be concise and focus on clinically significant findings.\"\"\"\n",
    "\n",
    "def create_triage_messages(image: Image.Image) -> list:\n",
    "    \"\"\"Create chat messages for triage inference.\"\"\"\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT}]\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": TRIAGE_PROMPT},\n",
    "                {\"type\": \"image\", \"image\": image}\n",
    "            ]\n",
    "        }\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Inference on Sample Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def run_triage_inference(pipe, image: Image.Image, max_tokens: int = 300) -> dict:\n",
    "    \"\"\"Run triage inference on a single image.\"\"\"\n",
    "    messages = create_triage_messages(image)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    output = pipe(text=messages, max_new_tokens=max_tokens)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    response = output[0][\"generated_text\"][-1][\"content\"]\n",
    "    \n",
    "    return {\n",
    "        \"response\": response,\n",
    "        \"inference_time_ms\": inference_time * 1000\n",
    "    }\n",
    "\n",
    "\n",
    "# Run inference on all samples\n",
    "print(\"=\" * 60)\n",
    "print(\"MedGemma CXR Triage Inference\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results = []\n",
    "for i, sample in enumerate(images):\n",
    "    print(f\"\\n[Image {i+1}/{len(images)}] {sample['name']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    result = run_triage_inference(pipe, sample[\"image\"])\n",
    "    results.append({\n",
    "        \"name\": sample[\"name\"],\n",
    "        \"expected\": sample[\"expected\"],\n",
    "        **result\n",
    "    })\n",
    "    \n",
    "    print(f\"Inference time: {result['inference_time_ms']:.0f}ms\")\n",
    "    print(f\"\\nResponse:\\n{result['response']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Inference Complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Parse and Structure Results\n",
    "\n",
    "Extract structured fields from the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse_triage_response(response: str) -> dict:\n",
    "    \"\"\"Parse structured fields from triage response.\"\"\"\n",
    "    result = {\n",
    "        \"urgency\": None,\n",
    "        \"explanation\": None,\n",
    "        \"key_findings\": []\n",
    "    }\n",
    "    \n",
    "    # Extract urgency\n",
    "    urgency_match = re.search(r'URGENCY:\\s*\\[?(Urgent|Non-Urgent)\\]?', response, re.IGNORECASE)\n",
    "    if urgency_match:\n",
    "        result[\"urgency\"] = urgency_match.group(1).capitalize()\n",
    "    \n",
    "    # Extract explanation\n",
    "    explanation_match = re.search(r'EXPLANATION:\\s*\\[?([^\\]\\n]+)\\]?', response, re.IGNORECASE)\n",
    "    if explanation_match:\n",
    "        result[\"explanation\"] = explanation_match.group(1).strip()\n",
    "    \n",
    "    # Extract key findings\n",
    "    findings_section = re.search(r'KEY FINDINGS:\\s*(.+)', response, re.IGNORECASE | re.DOTALL)\n",
    "    if findings_section:\n",
    "        findings_text = findings_section.group(1)\n",
    "        # Split by bullet points or numbers\n",
    "        findings = re.split(r'[\\n•\\-\\d\\.]+', findings_text)\n",
    "        result[\"key_findings\"] = [f.strip() for f in findings if f.strip()]\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# Parse all results\n",
    "for result in results:\n",
    "    parsed = parse_triage_response(result[\"response\"])\n",
    "    result.update(parsed)\n",
    "\n",
    "# Display structured results\n",
    "print(\"Structured Results:\")\n",
    "print(\"-\" * 40)\n",
    "for result in results:\n",
    "    print(f\"Image: {result['name']}\")\n",
    "    print(f\"  Urgency: {result['urgency']}\")\n",
    "    print(f\"  Explanation: {result['explanation']}\")\n",
    "    print(f\"  Key Findings: {result['key_findings'][:3]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Next Steps\n",
    "\n",
    "### What we demonstrated:\n",
    "- Loading MedGemma 4B IT model\n",
    "- Running multimodal inference (image + text)\n",
    "- Parsing structured triage outputs\n",
    "\n",
    "### Performance:\n",
    "- Inference time varies by GPU (typically 1-5 seconds)\n",
    "- Memory usage: ~8GB VRAM in bfloat16\n",
    "\n",
    "### Next Steps:\n",
    "1. **Fine-tuning**: See `02_fine_tune_medgemma.ipynb` for task-specific training\n",
    "2. **Evaluation**: See `03_evaluation_and_metrics.ipynb` for comprehensive metrics\n",
    "3. **Deployment**: See `demo_app/` for production inference\n",
    "\n",
    "### ⚠️ Disclaimer\n",
    "This is for **clinical decision support only**. All AI outputs require verification by qualified healthcare professionals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results for evaluation\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(\"../eval\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save inference results\n",
    "output_file = output_dir / \"quickstart_results.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    # Remove PIL images before serializing\n",
    "    serializable_results = [\n",
    "        {k: v for k, v in r.items() if k != \"image\"} \n",
    "        for r in results\n",
    "    ]\n",
    "    json.dump(serializable_results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved to: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
