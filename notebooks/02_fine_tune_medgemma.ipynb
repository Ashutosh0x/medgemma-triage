{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# MedGemma Fine-Tuning for CXR Triage\n",
                "\n",
                "This notebook demonstrates how to fine-tune MedGemma 4B for chest X-ray urgency classification.\n",
                "\n",
                "**Requirements:**\n",
                "- GPU with 24GB+ VRAM (for full fine-tuning) or 16GB+ (with LoRA)\n",
                "- MIMIC-CXR dataset access (see `data_scripts/download_datasets.sh`)\n",
                "- `transformers>=4.50.0`, `peft>=0.7.0`\n",
                "\n",
                "**Time to complete:** ~2-4 hours (depending on dataset size)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup & Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "# !pip install -U transformers accelerate peft torch bitsandbytes\n",
                "\n",
                "import os\n",
                "import json\n",
                "import torch\n",
                "from pathlib import Path\n",
                "from dataclasses import dataclass\n",
                "from typing import Optional, Dict, List, Any\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "print(f\"PyTorch: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@dataclass\n",
                "class TrainingConfig:\n",
                "    \"\"\"Training configuration for MedGemma fine-tuning.\"\"\"\n",
                "    # Model\n",
                "    model_id: str = \"google/medgemma-4b-it\"\n",
                "    \n",
                "    # Data\n",
                "    data_dir: str = \"../data/processed\"\n",
                "    max_samples: Optional[int] = None  # None = use all\n",
                "    \n",
                "    # LoRA Configuration\n",
                "    use_lora: bool = True\n",
                "    lora_r: int = 16\n",
                "    lora_alpha: int = 32\n",
                "    lora_dropout: float = 0.05\n",
                "    lora_target_modules: List[str] = None  # Auto-detect if None\n",
                "    \n",
                "    # Training Hyperparameters\n",
                "    learning_rate: float = 2e-5\n",
                "    num_epochs: int = 3\n",
                "    batch_size: int = 2\n",
                "    gradient_accumulation_steps: int = 8\n",
                "    warmup_ratio: float = 0.1\n",
                "    weight_decay: float = 0.01\n",
                "    \n",
                "    # Generation\n",
                "    max_new_tokens: int = 256\n",
                "    \n",
                "    # Output\n",
                "    output_dir: str = \"../models/checkpoints/urgency_classifier\"\n",
                "    logging_steps: int = 10\n",
                "    save_steps: int = 100\n",
                "    eval_steps: int = 50\n",
                "    \n",
                "    def __post_init__(self):\n",
                "        if self.lora_target_modules is None:\n",
                "            # Target attention and MLP layers for LoRA\n",
                "            self.lora_target_modules = [\n",
                "                \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "                \"gate_proj\", \"up_proj\", \"down_proj\"\n",
                "            ]\n",
                "\n",
                "config = TrainingConfig()\n",
                "print(\"Training Configuration:\")\n",
                "for k, v in config.__dict__.items():\n",
                "    print(f\"  {k}: {v}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Model with LoRA\n",
                "\n",
                "We use PEFT (Parameter-Efficient Fine-Tuning) with LoRA to reduce memory requirements."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import (\n",
                "    AutoModelForImageTextToText,\n",
                "    AutoProcessor,\n",
                "    BitsAndBytesConfig,\n",
                ")\n",
                "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
                "\n",
                "# Quantization config for memory efficiency\n",
                "bnb_config = BitsAndBytesConfig(\n",
                "    load_in_4bit=True,\n",
                "    bnb_4bit_quant_type=\"nf4\",\n",
                "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
                "    bnb_4bit_use_double_quant=True,\n",
                ")\n",
                "\n",
                "print(f\"Loading model: {config.model_id}\")\n",
                "\n",
                "# Load base model\n",
                "model = AutoModelForImageTextToText.from_pretrained(\n",
                "    config.model_id,\n",
                "    quantization_config=bnb_config if config.use_lora else None,\n",
                "    torch_dtype=torch.bfloat16,\n",
                "    device_map=\"auto\",\n",
                ")\n",
                "\n",
                "# Load processor\n",
                "processor = AutoProcessor.from_pretrained(config.model_id)\n",
                "\n",
                "print(f\"Model loaded. Parameters: {model.num_parameters():,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if config.use_lora:\n",
                "    print(\"Applying LoRA configuration...\")\n",
                "    \n",
                "    # Prepare model for k-bit training\n",
                "    model = prepare_model_for_kbit_training(model)\n",
                "    \n",
                "    # LoRA configuration\n",
                "    lora_config = LoraConfig(\n",
                "        r=config.lora_r,\n",
                "        lora_alpha=config.lora_alpha,\n",
                "        lora_dropout=config.lora_dropout,\n",
                "        target_modules=config.lora_target_modules,\n",
                "        bias=\"none\",\n",
                "        task_type=\"CAUSAL_LM\",\n",
                "    )\n",
                "    \n",
                "    # Apply LoRA\n",
                "    model = get_peft_model(model, lora_config)\n",
                "    \n",
                "    # Print trainable parameters\n",
                "    model.print_trainable_parameters()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Prepare Dataset\n",
                "\n",
                "Load and preprocess the MIMIC-CXR subset for training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from torch.utils.data import Dataset, DataLoader\n",
                "from PIL import Image\n",
                "import random\n",
                "\n",
                "class CXRTriageDataset(Dataset):\n",
                "    \"\"\"Dataset for CXR triage training.\"\"\"\n",
                "    \n",
                "    SYSTEM_PROMPT = \"\"\"You are an expert radiologist assistant. Analyze chest X-rays and provide:\n",
                "1. Urgency classification (Urgent or Non-Urgent)\n",
                "2. Brief explanation\n",
                "3. Key findings\n",
                "\n",
                "This is for clinical decision support only.\"\"\"\n",
                "    \n",
                "    def __init__(\n",
                "        self,\n",
                "        data_file: str,\n",
                "        processor,\n",
                "        max_samples: Optional[int] = None,\n",
                "        use_sample_data: bool = False,\n",
                "    ):\n",
                "        self.processor = processor\n",
                "        self.samples = []\n",
                "        \n",
                "        if use_sample_data:\n",
                "            # Create synthetic samples for testing\n",
                "            self.samples = self._create_sample_data()\n",
                "        else:\n",
                "            # Load from JSONL file\n",
                "            data_path = Path(data_file)\n",
                "            if data_path.exists():\n",
                "                with open(data_path) as f:\n",
                "                    for line in f:\n",
                "                        self.samples.append(json.loads(line))\n",
                "            else:\n",
                "                print(f\"Warning: Data file not found: {data_file}\")\n",
                "                print(\"Using sample data for demonstration.\")\n",
                "                self.samples = self._create_sample_data()\n",
                "        \n",
                "        if max_samples:\n",
                "            self.samples = self.samples[:max_samples]\n",
                "        \n",
                "        print(f\"Dataset loaded: {len(self.samples)} samples\")\n",
                "    \n",
                "    def _create_sample_data(self) -> List[Dict]:\n",
                "        \"\"\"Create sample data for testing the training pipeline.\"\"\"\n",
                "        return [\n",
                "            {\n",
                "                \"id\": \"sample_1\",\n",
                "                \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png\",\n",
                "                \"urgency\": \"non-urgent\",\n",
                "                \"primary_finding\": \"No Finding\",\n",
                "                \"explanation\": \"Normal chest X-ray with clear lung fields.\",\n",
                "            },\n",
                "        ] * 10  # Repeat for minimal training\n",
                "    \n",
                "    def __len__(self) -> int:\n",
                "        return len(self.samples)\n",
                "    \n",
                "    def _load_image(self, sample: Dict) -> Image.Image:\n",
                "        \"\"\"Load image from path or URL.\"\"\"\n",
                "        import requests\n",
                "        from io import BytesIO\n",
                "        \n",
                "        if \"image_url\" in sample:\n",
                "            response = requests.get(sample[\"image_url\"], headers={\"User-Agent\": \"MedGemma\"})\n",
                "            return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
                "        elif \"image_path\" in sample:\n",
                "            return Image.open(sample[\"image_path\"]).convert(\"RGB\")\n",
                "        else:\n",
                "            raise ValueError(\"No image source found in sample\")\n",
                "    \n",
                "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
                "        sample = self.samples[idx]\n",
                "        \n",
                "        # Load image\n",
                "        image = self._load_image(sample)\n",
                "        \n",
                "        # Create target response\n",
                "        urgency = sample[\"urgency\"].capitalize()\n",
                "        if urgency == \"Non-urgent\":\n",
                "            urgency = \"Non-Urgent\"\n",
                "        \n",
                "        explanation = sample.get(\"explanation\", f\"{sample['primary_finding']} detected.\")\n",
                "        \n",
                "        target_response = f\"\"\"1. URGENCY: [{urgency}]\n",
                "2. EXPLANATION: [{explanation}]\n",
                "3. KEY FINDINGS: [{sample['primary_finding']}]\"\"\"\n",
                "        \n",
                "        # Create messages\n",
                "        messages = [\n",
                "            {\n",
                "                \"role\": \"system\",\n",
                "                \"content\": [{\"type\": \"text\", \"text\": self.SYSTEM_PROMPT}]\n",
                "            },\n",
                "            {\n",
                "                \"role\": \"user\",\n",
                "                \"content\": [\n",
                "                    {\"type\": \"text\", \"text\": \"Analyze this chest X-ray for triage.\"},\n",
                "                    {\"type\": \"image\", \"image\": image}\n",
                "                ]\n",
                "            },\n",
                "            {\n",
                "                \"role\": \"assistant\",\n",
                "                \"content\": [{\"type\": \"text\", \"text\": target_response}]\n",
                "            }\n",
                "        ]\n",
                "        \n",
                "        return {\n",
                "            \"messages\": messages,\n",
                "            \"image\": image,\n",
                "            \"label\": sample[\"urgency\"],\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create datasets\n",
                "train_dataset = CXRTriageDataset(\n",
                "    data_file=f\"{config.data_dir}/train.jsonl\",\n",
                "    processor=processor,\n",
                "    max_samples=config.max_samples,\n",
                "    use_sample_data=True,  # Set to False when using real data\n",
                ")\n",
                "\n",
                "val_dataset = CXRTriageDataset(\n",
                "    data_file=f\"{config.data_dir}/val.jsonl\",\n",
                "    processor=processor,\n",
                "    max_samples=config.max_samples // 10 if config.max_samples else None,\n",
                "    use_sample_data=True,\n",
                ")\n",
                "\n",
                "print(f\"Train samples: {len(train_dataset)}\")\n",
                "print(f\"Val samples: {len(val_dataset)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Training Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from transformers import TrainingArguments, Trainer\n",
                "from tqdm import tqdm\n",
                "\n",
                "# Create output directory\n",
                "output_dir = Path(config.output_dir)\n",
                "output_dir.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "# Training arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=str(output_dir),\n",
                "    num_train_epochs=config.num_epochs,\n",
                "    per_device_train_batch_size=config.batch_size,\n",
                "    per_device_eval_batch_size=config.batch_size,\n",
                "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
                "    learning_rate=config.learning_rate,\n",
                "    warmup_ratio=config.warmup_ratio,\n",
                "    weight_decay=config.weight_decay,\n",
                "    logging_steps=config.logging_steps,\n",
                "    save_steps=config.save_steps,\n",
                "    eval_steps=config.eval_steps,\n",
                "    evaluation_strategy=\"steps\",\n",
                "    save_total_limit=2,\n",
                "    load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"eval_loss\",\n",
                "    greater_is_better=False,\n",
                "    bf16=True,\n",
                "    dataloader_pin_memory=False,\n",
                "    report_to=\"none\",  # Disable wandb/tensorboard for simplicity\n",
                ")\n",
                "\n",
                "print(\"Training arguments configured.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def collate_fn(batch):\n",
                "    \"\"\"Custom collate function for multimodal data.\"\"\"\n",
                "    # Process each sample\n",
                "    processed_batch = []\n",
                "    \n",
                "    for sample in batch:\n",
                "        # Apply chat template and tokenize\n",
                "        inputs = processor.apply_chat_template(\n",
                "            sample[\"messages\"],\n",
                "            add_generation_prompt=False,\n",
                "            tokenize=True,\n",
                "            return_dict=True,\n",
                "            return_tensors=\"pt\",\n",
                "        )\n",
                "        \n",
                "        # Create labels (same as input_ids for causal LM)\n",
                "        inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
                "        \n",
                "        processed_batch.append(inputs)\n",
                "    \n",
                "    # Stack tensors\n",
                "    return {\n",
                "        \"input_ids\": torch.cat([b[\"input_ids\"] for b in processed_batch]),\n",
                "        \"attention_mask\": torch.cat([b[\"attention_mask\"] for b in processed_batch]),\n",
                "        \"labels\": torch.cat([b[\"labels\"] for b in processed_batch]),\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Run Training\n",
                "\n",
                "⚠️ **Note**: This is a simplified training loop for demonstration. For production training, use the full `Trainer` with proper data collation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simple training loop for demonstration\n",
                "print(\"=\" * 60)\n",
                "print(\"Starting Fine-Tuning (Demonstration Mode)\")\n",
                "print(\"=\" * 60)\n",
                "print()\n",
                "print(\"Note: This notebook demonstrates the training setup.\")\n",
                "print(\"For full training, ensure you have:\")\n",
                "print(\"  1. MIMIC-CXR dataset downloaded and processed\")\n",
                "print(\"  2. Sufficient GPU memory (24GB+ recommended)\")\n",
                "print(\"  3. Several hours of training time\")\n",
                "print()\n",
                "\n",
                "# Save configuration\n",
                "config_file = output_dir / \"training_config.json\"\n",
                "with open(config_file, \"w\") as f:\n",
                "    json.dump(config.__dict__, f, indent=2, default=str)\n",
                "print(f\"Configuration saved to: {config_file}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Optional: Run a single training step to verify setup\n",
                "print(\"\\nVerifying training setup with single batch...\")\n",
                "\n",
                "try:\n",
                "    # Get a single sample\n",
                "    sample = train_dataset[0]\n",
                "    \n",
                "    # Process the sample\n",
                "    inputs = processor.apply_chat_template(\n",
                "        sample[\"messages\"],\n",
                "        add_generation_prompt=False,\n",
                "        tokenize=True,\n",
                "        return_dict=True,\n",
                "        return_tensors=\"pt\",\n",
                "    )\n",
                "    \n",
                "    # Move to device\n",
                "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
                "    inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n",
                "    \n",
                "    # Forward pass\n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs)\n",
                "        loss = outputs.loss\n",
                "    \n",
                "    print(f\"✓ Forward pass successful! Loss: {loss.item():.4f}\")\n",
                "    print(f\"✓ Input shape: {inputs['input_ids'].shape}\")\n",
                "    print(\"\\nSetup verified. Ready for full training.\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"✗ Error during verification: {e}\")\n",
                "    raise"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Save LoRA Adapter\n",
                "\n",
                "Save the trained LoRA adapter for inference."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the LoRA adapter (after training)\n",
                "if config.use_lora:\n",
                "    print(\"Saving LoRA adapter...\")\n",
                "    model.save_pretrained(output_dir)\n",
                "    processor.save_pretrained(output_dir)\n",
                "    print(f\"Adapter saved to: {output_dir}\")\n",
                "else:\n",
                "    print(\"Full model fine-tuning - saving complete model...\")\n",
                "    model.save_pretrained(output_dir)\n",
                "    processor.save_pretrained(output_dir)\n",
                "\n",
                "# Save training summary\n",
                "summary = {\n",
                "    \"model_id\": config.model_id,\n",
                "    \"training_samples\": len(train_dataset),\n",
                "    \"val_samples\": len(val_dataset),\n",
                "    \"epochs\": config.num_epochs,\n",
                "    \"learning_rate\": config.learning_rate,\n",
                "    \"use_lora\": config.use_lora,\n",
                "    \"lora_r\": config.lora_r if config.use_lora else None,\n",
                "}\n",
                "\n",
                "with open(output_dir / \"training_summary.json\", \"w\") as f:\n",
                "    json.dump(summary, f, indent=2)\n",
                "\n",
                "print(\"\\nTraining Summary:\")\n",
                "for k, v in summary.items():\n",
                "    print(f\"  {k}: {v}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Next Steps\n",
                "\n",
                "After training, proceed to:\n",
                "1. **Evaluation**: Run `03_evaluation_and_metrics.ipynb` to compute AUC, sensitivity, PPV\n",
                "2. **Demo**: Export to ONNX and deploy via `demo_app/`\n",
                "\n",
                "### Hyperparameter Tuning Tips\n",
                "\n",
                "| Parameter | Recommendation |\n",
                "|-----------|---------------|\n",
                "| `lora_r` | Start with 16, increase to 32 if underfitting |\n",
                "| `learning_rate` | 1e-5 to 5e-5 works well for LoRA |\n",
                "| `epochs` | 3-5 epochs usually sufficient |\n",
                "| `batch_size` | As large as GPU allows (effective batch = batch × grad_accum) |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}