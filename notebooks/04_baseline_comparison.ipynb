{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Baseline Comparison & Calibration Analysis\n",
                "\n",
                "This notebook compares MedGemma 1.5 against baseline models to validate its claims:\n",
                "\n",
                "**Baselines:**\n",
                "- DenseNet-121 (CheXNet-style) for image classification\n",
                "- Whisper-base for ASR (vs MedASR)\n",
                "- Non-medical LLM for QA (optional)\n",
                "\n",
                "**Calibration:**\n",
                "- Expected Calibration Error (ECE)\n",
                "- Brier Score\n",
                "- Reliability Diagram\n",
                "\n",
                "**Time to complete:** ~20 minutes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "from typing import Dict, List, Tuple\n",
                "from sklearn.metrics import roc_auc_score, brier_score_loss\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('husl')\n",
                "\n",
                "EVAL_DIR = Path(\"../eval\")\n",
                "print(\"Baseline comparison notebook ready.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define Baseline Models\n",
                "\n",
                "We compare MedGemma 1.5 against:\n",
                "\n",
                "| Model | Type | Purpose |\n",
                "|-------|------|--------|\n",
                "| DenseNet-121 | CNN | CXR classification baseline |\n",
                "| MedGemma 1.5 4B | VLM | Our model |\n",
                "| Whisper-base | ASR | Speech baseline (vs MedASR) |"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Simulated baseline results for demonstration\n",
                "# In practice, run each model on the same test set\n",
                "\n",
                "np.random.seed(42)\n",
                "n_samples = 100\n",
                "\n",
                "# Ground truth labels (50% urgent)\n",
                "y_true = np.array([1] * 50 + [0] * 50)\n",
                "np.random.shuffle(y_true)\n",
                "\n",
                "def simulate_model_predictions(auc_target: float, calibration: str = \"good\") -> np.ndarray:\n",
                "    \"\"\"Simulate model predictions with target AUC and calibration.\"\"\"\n",
                "    # Generate scores that achieve target AUC\n",
                "    scores = np.zeros(n_samples)\n",
                "    for i in range(n_samples):\n",
                "        if y_true[i] == 1:\n",
                "            # Higher scores for positive class\n",
                "            scores[i] = np.clip(np.random.beta(auc_target * 5, 2), 0, 1)\n",
                "        else:\n",
                "            # Lower scores for negative class\n",
                "            scores[i] = np.clip(np.random.beta(2, auc_target * 5), 0, 1)\n",
                "    \n",
                "    # Apply calibration adjustment\n",
                "    if calibration == \"overconfident\":\n",
                "        scores = np.where(scores > 0.5, scores * 1.2, scores * 0.8)\n",
                "    elif calibration == \"underconfident\":\n",
                "        scores = scores * 0.7 + 0.15\n",
                "    \n",
                "    return np.clip(scores, 0, 1)\n",
                "\n",
                "# Simulate predictions for each model\n",
                "baselines = {\n",
                "    \"DenseNet-121 (CheXNet)\": {\n",
                "        \"scores\": simulate_model_predictions(0.82, \"overconfident\"),\n",
                "        \"type\": \"CNN\",\n",
                "        \"params\": \"8M\",\n",
                "    },\n",
                "    \"MedGemma 1.5 (4B)\": {\n",
                "        \"scores\": simulate_model_predictions(0.91, \"good\"),\n",
                "        \"type\": \"VLM\",\n",
                "        \"params\": \"4B\",\n",
                "    },\n",
                "}\n",
                "\n",
                "print(\"Baseline models defined.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Compute Comparison Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_ece(y_true: np.ndarray, y_prob: np.ndarray, n_bins: int = 10) -> float:\n",
                "    \"\"\"Compute Expected Calibration Error.\"\"\"\n",
                "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
                "    ece = 0.0\n",
                "    \n",
                "    for i in range(n_bins):\n",
                "        bin_lower = bin_boundaries[i]\n",
                "        bin_upper = bin_boundaries[i + 1]\n",
                "        \n",
                "        # Samples in this bin\n",
                "        in_bin = (y_prob > bin_lower) & (y_prob <= bin_upper)\n",
                "        n_in_bin = np.sum(in_bin)\n",
                "        \n",
                "        if n_in_bin > 0:\n",
                "            # Average confidence and accuracy in this bin\n",
                "            avg_confidence = np.mean(y_prob[in_bin])\n",
                "            avg_accuracy = np.mean(y_true[in_bin])\n",
                "            \n",
                "            # Weighted calibration error\n",
                "            ece += (n_in_bin / len(y_true)) * np.abs(avg_accuracy - avg_confidence)\n",
                "    \n",
                "    return ece\n",
                "\n",
                "\n",
                "def get_reliability_data(y_true: np.ndarray, y_prob: np.ndarray, n_bins: int = 10) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
                "    \"\"\"Get data for reliability diagram.\"\"\"\n",
                "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
                "    bin_centers = []\n",
                "    accuracies = []\n",
                "    counts = []\n",
                "    \n",
                "    for i in range(n_bins):\n",
                "        bin_lower = bin_boundaries[i]\n",
                "        bin_upper = bin_boundaries[i + 1]\n",
                "        \n",
                "        in_bin = (y_prob > bin_lower) & (y_prob <= bin_upper)\n",
                "        n_in_bin = np.sum(in_bin)\n",
                "        \n",
                "        if n_in_bin > 0:\n",
                "            bin_centers.append((bin_lower + bin_upper) / 2)\n",
                "            accuracies.append(np.mean(y_true[in_bin]))\n",
                "            counts.append(n_in_bin)\n",
                "    \n",
                "    return np.array(bin_centers), np.array(accuracies), np.array(counts)\n",
                "\n",
                "\n",
                "# Compute metrics for all models\n",
                "results = []\n",
                "for name, model_data in baselines.items():\n",
                "    scores = model_data[\"scores\"]\n",
                "    y_pred = (scores > 0.5).astype(int)\n",
                "    \n",
                "    auc = roc_auc_score(y_true, scores)\n",
                "    brier = brier_score_loss(y_true, scores)\n",
                "    ece = compute_ece(y_true, scores)\n",
                "    \n",
                "    # Sensitivity at default threshold\n",
                "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
                "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
                "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
                "    \n",
                "    results.append({\n",
                "        \"Model\": name,\n",
                "        \"Type\": model_data[\"type\"],\n",
                "        \"Params\": model_data[\"params\"],\n",
                "        \"AUC-ROC\": auc,\n",
                "        \"Sensitivity\": sensitivity,\n",
                "        \"Brier Score\": brier,\n",
                "        \"ECE\": ece,\n",
                "    })\n",
                "\n",
                "results_df = pd.DataFrame(results)\n",
                "print(\"\\n\" + \"=\" * 70)\n",
                "print(\"BASELINE COMPARISON RESULTS\")\n",
                "print(\"=\" * 70)\n",
                "print(results_df.to_string(index=False))\n",
                "print(\"=\" * 70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Relative Improvement Calculation\n",
                "\n",
                "Following proper reporting: **both absolute and relative changes**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate improvements vs baseline\n",
                "baseline_name = \"DenseNet-121 (CheXNet)\"\n",
                "target_name = \"MedGemma 1.5 (4B)\"\n",
                "\n",
                "baseline_auc = results_df[results_df[\"Model\"] == baseline_name][\"AUC-ROC\"].values[0]\n",
                "target_auc = results_df[results_df[\"Model\"] == target_name][\"AUC-ROC\"].values[0]\n",
                "\n",
                "abs_improvement = target_auc - baseline_auc\n",
                "rel_improvement = (target_auc - baseline_auc) / baseline_auc * 100\n",
                "\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"AUC-ROC IMPROVEMENT ANALYSIS\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Baseline ({baseline_name}): {baseline_auc:.3f}\")\n",
                "print(f\"MedGemma 1.5:               {target_auc:.3f}\")\n",
                "print(\"-\" * 50)\n",
                "print(f\"Absolute improvement:       +{abs_improvement:.3f} points\")\n",
                "print(f\"Relative improvement:       +{rel_improvement:.1f}%\")\n",
                "print(\"=\" * 50)\n",
                "print()\n",
                "print(f'Writeup format: \"AUC improved from {baseline_auc:.3f} → {target_auc:.3f} ')\n",
                "print(f'                 (absolute +{abs_improvement:.3f}, relative +{rel_improvement:.1f}%)\"')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Calibration Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# 1. Reliability Diagrams (side by side)\n",
                "ax1 = axes[0]\n",
                "colors = ['#e74c3c', '#27ae60']\n",
                "\n",
                "for idx, (name, model_data) in enumerate(baselines.items()):\n",
                "    bin_centers, accuracies, counts = get_reliability_data(y_true, model_data[\"scores\"])\n",
                "    ax1.plot(bin_centers, accuracies, 'o-', color=colors[idx], label=name, linewidth=2, markersize=8)\n",
                "\n",
                "# Perfect calibration line\n",
                "ax1.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration', alpha=0.5)\n",
                "\n",
                "ax1.set_xlabel('Mean Predicted Probability', fontsize=12)\n",
                "ax1.set_ylabel('Fraction of Positives', fontsize=12)\n",
                "ax1.set_title('Reliability Diagram', fontsize=14, fontweight='bold')\n",
                "ax1.legend(loc='lower right')\n",
                "ax1.set_xlim([0, 1])\n",
                "ax1.set_ylim([0, 1])\n",
                "\n",
                "# 2. Calibration Metrics Bar Chart\n",
                "ax2 = axes[1]\n",
                "x = np.arange(len(baselines))\n",
                "width = 0.35\n",
                "\n",
                "ece_values = results_df['ECE'].values\n",
                "brier_values = results_df['Brier Score'].values\n",
                "\n",
                "bars1 = ax2.bar(x - width/2, ece_values, width, label='ECE (↓ better)', color='#3498db')\n",
                "bars2 = ax2.bar(x + width/2, brier_values, width, label='Brier Score (↓ better)', color='#9b59b6')\n",
                "\n",
                "ax2.set_ylabel('Score', fontsize=12)\n",
                "ax2.set_title('Calibration Metrics Comparison', fontsize=14, fontweight='bold')\n",
                "ax2.set_xticks(x)\n",
                "ax2.set_xticklabels([name.split(' (')[0] for name in baselines.keys()], rotation=15, ha='right')\n",
                "ax2.legend()\n",
                "ax2.set_ylim([0, 0.4])\n",
                "\n",
                "# Add value labels\n",
                "for bar, val in zip(bars1, ece_values):\n",
                "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
                "for bar, val in zip(bars2, brier_values):\n",
                "    ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, f'{val:.3f}', ha='center', va='bottom', fontsize=10)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(EVAL_DIR / 'baseline_calibration.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"Saved to: {EVAL_DIR / 'baseline_calibration.png'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Threshold Tuning for High Sensitivity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Find threshold for ≥95% sensitivity\n",
                "medgemma_scores = baselines[\"MedGemma 1.5 (4B)\"][\"scores\"]\n",
                "\n",
                "thresholds = np.arange(0.0, 1.0, 0.01)\n",
                "sensitivities = []\n",
                "ppvs = []\n",
                "\n",
                "for thresh in thresholds:\n",
                "    y_pred = (medgemma_scores >= thresh).astype(int)\n",
                "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
                "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
                "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
                "    \n",
                "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
                "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
                "    \n",
                "    sensitivities.append(sens)\n",
                "    ppvs.append(ppv)\n",
                "\n",
                "sensitivities = np.array(sensitivities)\n",
                "ppvs = np.array(ppvs)\n",
                "\n",
                "# Find threshold for 95% sensitivity\n",
                "target_sensitivity = 0.95\n",
                "valid_idx = np.where(sensitivities >= target_sensitivity)[0]\n",
                "if len(valid_idx) > 0:\n",
                "    optimal_idx = valid_idx[-1]  # Highest threshold that achieves target\n",
                "    optimal_threshold = thresholds[optimal_idx]\n",
                "    optimal_ppv = ppvs[optimal_idx]\n",
                "    optimal_sens = sensitivities[optimal_idx]\n",
                "else:\n",
                "    optimal_threshold = 0.5\n",
                "    optimal_idx = 50\n",
                "    optimal_ppv = ppvs[optimal_idx]\n",
                "    optimal_sens = sensitivities[optimal_idx]\n",
                "\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"THRESHOLD TUNING FOR HIGH SENSITIVITY\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Target:           Sensitivity ≥ {target_sensitivity:.0%}\")\n",
                "print(f\"Optimal threshold: {optimal_threshold:.2f}\")\n",
                "print(f\"Achieved sensitivity: {optimal_sens:.2%}\")\n",
                "print(f\"PPV at this threshold: {optimal_ppv:.2%}\")\n",
                "print(\"=\" * 50)\n",
                "print()\n",
                "print(\"Interpretation:\")\n",
                "print(f\"  At threshold {optimal_threshold:.2f}, we detect {optimal_sens:.0%} of urgent cases\")\n",
                "print(f\"  with a positive predictive value of {optimal_ppv:.0%}.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Save Comparison Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save comparison results\n",
                "comparison_report = {\n",
                "    \"baseline\": baseline_name,\n",
                "    \"target\": target_name,\n",
                "    \"n_samples\": int(n_samples),\n",
                "    \"results\": results_df.to_dict(orient='records'),\n",
                "    \"improvement\": {\n",
                "        \"auc_absolute\": float(abs_improvement),\n",
                "        \"auc_relative_pct\": float(rel_improvement),\n",
                "    },\n",
                "    \"threshold_tuning\": {\n",
                "        \"target_sensitivity\": float(target_sensitivity),\n",
                "        \"optimal_threshold\": float(optimal_threshold),\n",
                "        \"achieved_sensitivity\": float(optimal_sens),\n",
                "        \"ppv_at_threshold\": float(optimal_ppv),\n",
                "    },\n",
                "}\n",
                "\n",
                "with open(EVAL_DIR / \"baseline_comparison.json\", \"w\") as f:\n",
                "    json.dump(comparison_report, f, indent=2)\n",
                "\n",
                "print(f\"Report saved to: {EVAL_DIR / 'baseline_comparison.json'}\")\n",
                "print()\n",
                "print(\"=\" * 60)\n",
                "print(\"SUMMARY FOR WRITEUP\")\n",
                "print(\"=\" * 60)\n",
                "print(f\"MedGemma 1.5 achieves AUC-ROC of {target_auc:.3f}, representing a\")\n",
                "print(f\"+{abs_improvement:.3f} absolute (+{rel_improvement:.1f}% relative) improvement\")\n",
                "print(f\"over the DenseNet-121 baseline.\")\n",
                "print()\n",
                "print(f\"With threshold tuned for ≥95% sensitivity, we achieve {optimal_sens:.0%}\")\n",
                "print(f\"sensitivity with {optimal_ppv:.0%} PPV.\")\n",
                "print(\"=\" * 60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Key Takeaways\n",
                "\n",
                "### Why MedGemma is Better\n",
                "\n",
                "1. **Higher AUC**: MedGemma 1.5 outperforms the CNN baseline\n",
                "2. **Better Calibration**: Lower ECE means more reliable confidence scores\n",
                "3. **Explainability**: VLM provides textual rationale, CNN does not\n",
                "4. **Multimodal**: Can incorporate prior reports, CNN is image-only\n",
                "\n",
                "### Threshold Recommendation\n",
                "\n",
                "For clinical triage, prioritize **sensitivity** (detect urgent cases):\n",
                "- Use threshold that achieves ≥95% sensitivity\n",
                "- Accept lower PPV in exchange for safety\n",
                "\n",
                "### ⚠️ Note\n",
                "Results in this notebook are simulated for demonstration.\n",
                "Replace with actual model predictions for final submission."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}