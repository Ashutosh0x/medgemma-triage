{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# MedGemma Evaluation & Metrics\n",
                "\n",
                "This notebook evaluates the fine-tuned MedGemma model for CXR triage using:\n",
                "- **AUC-ROC**: Classification performance\n",
                "- **Sensitivity @ 95% Recall**: Detecting urgent cases\n",
                "- **PPV (Positive Predictive Value)**: Precision for urgent predictions\n",
                "- **Clinician Rating**: Human evaluation of explanations\n",
                "\n",
                "**Time to complete:** ~30 minutes"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "from typing import Dict, List, Tuple\n",
                "from sklearn.metrics import (\n",
                "    roc_auc_score, \n",
                "    precision_recall_curve,\n",
                "    confusion_matrix,\n",
                "    classification_report\n",
                ")\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "# Set style\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('husl')\n",
                "\n",
                "print(\"Evaluation notebook ready.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Load Predictions and Ground Truth"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Path configuration\n",
                "EVAL_DIR = Path(\"../eval\")\n",
                "DATA_DIR = Path(\"../data/processed\")\n",
                "\n",
                "# Load or create sample data\n",
                "def load_jsonl(filepath: Path) -> List[Dict]:\n",
                "    \"\"\"Load JSONL file.\"\"\"\n",
                "    if not filepath.exists():\n",
                "        return []\n",
                "    with open(filepath) as f:\n",
                "        return [json.loads(line) for line in f]\n",
                "\n",
                "\n",
                "def create_sample_predictions() -> Tuple[List[Dict], List[Dict]]:\n",
                "    \"\"\"Create sample predictions for demonstration.\"\"\"\n",
                "    np.random.seed(42)\n",
                "    \n",
                "    # Generate 100 sample predictions\n",
                "    n_samples = 100\n",
                "    \n",
                "    # Ground truth (50% urgent, 50% non-urgent)\n",
                "    labels = [\"urgent\"] * 50 + [\"non-urgent\"] * 50\n",
                "    np.random.shuffle(labels)\n",
                "    \n",
                "    gold = []\n",
                "    preds = []\n",
                "    \n",
                "    for i, label in enumerate(labels):\n",
                "        # Generate prediction with some error\n",
                "        if label == \"urgent\":\n",
                "            # 90% correct for urgent\n",
                "            pred_label = \"urgent\" if np.random.random() < 0.90 else \"non-urgent\"\n",
                "            score = np.clip(np.random.normal(0.8, 0.15), 0, 1)\n",
                "        else:\n",
                "            # 85% correct for non-urgent\n",
                "            pred_label = \"non-urgent\" if np.random.random() < 0.85 else \"urgent\"\n",
                "            score = np.clip(np.random.normal(0.3, 0.15), 0, 1)\n",
                "        \n",
                "        gold.append({\n",
                "            \"id\": f\"sample_{i:03d}\",\n",
                "            \"urgency\": label,\n",
                "            \"primary_finding\": \"Pneumonia\" if label == \"urgent\" else \"No Finding\",\n",
                "        })\n",
                "        \n",
                "        preds.append({\n",
                "            \"id\": f\"sample_{i:03d}\",\n",
                "            \"predicted_urgency\": pred_label,\n",
                "            \"confidence\": score,\n",
                "            \"explanation\": f\"Sample explanation for case {i}.\",\n",
                "        })\n",
                "    \n",
                "    return gold, preds\n",
                "\n",
                "\n",
                "# Try to load real data, fall back to sample\n",
                "gold_file = DATA_DIR / \"test.jsonl\"\n",
                "pred_file = EVAL_DIR / \"predictions.jsonl\"\n",
                "\n",
                "gold = load_jsonl(gold_file)\n",
                "preds = load_jsonl(pred_file)\n",
                "\n",
                "if not gold or not preds:\n",
                "    print(\"Using sample data for demonstration...\")\n",
                "    gold, preds = create_sample_predictions()\n",
                "\n",
                "print(f\"Gold labels: {len(gold)}\")\n",
                "print(f\"Predictions: {len(preds)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Prepare Data for Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create DataFrames\n",
                "gold_df = pd.DataFrame(gold)\n",
                "preds_df = pd.DataFrame(preds)\n",
                "\n",
                "# Merge on ID\n",
                "df = gold_df.merge(preds_df, on=\"id\", how=\"inner\")\n",
                "\n",
                "# Binary labels (1 = urgent, 0 = non-urgent)\n",
                "df[\"y_true\"] = (df[\"urgency\"] == \"urgent\").astype(int)\n",
                "df[\"y_pred\"] = (df[\"predicted_urgency\"] == \"urgent\").astype(int)\n",
                "df[\"y_score\"] = df[\"confidence\"]\n",
                "\n",
                "print(f\"Merged samples: {len(df)}\")\n",
                "print(f\"\\nLabel distribution:\")\n",
                "print(df[\"urgency\"].value_counts())\n",
                "print(f\"\\nPrediction distribution:\")\n",
                "print(df[\"predicted_urgency\"].value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Compute Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray, y_score: np.ndarray) -> Dict:\n",
                "    \"\"\"Compute all evaluation metrics.\"\"\"\n",
                "    \n",
                "    # AUC-ROC\n",
                "    auc = roc_auc_score(y_true, y_score)\n",
                "    \n",
                "    # Precision-Recall curve\n",
                "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_score)\n",
                "    \n",
                "    # Sensitivity at 95% recall\n",
                "    target_recall = 0.95\n",
                "    idx = np.argmin(np.abs(recalls - target_recall))\n",
                "    sensitivity_at_95_recall = precisions[idx]\n",
                "    threshold_at_95_recall = thresholds[idx] if idx < len(thresholds) else 0.5\n",
                "    \n",
                "    # Confusion matrix\n",
                "    cm = confusion_matrix(y_true, y_pred)\n",
                "    tn, fp, fn, tp = cm.ravel()\n",
                "    \n",
                "    # PPV (Positive Predictive Value = Precision)\n",
                "    ppv = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
                "    \n",
                "    # NPV (Negative Predictive Value)\n",
                "    npv = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
                "    \n",
                "    # Sensitivity (Recall for positive class)\n",
                "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
                "    \n",
                "    # Specificity\n",
                "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
                "    \n",
                "    # Accuracy\n",
                "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
                "    \n",
                "    return {\n",
                "        \"auc_roc\": auc,\n",
                "        \"sensitivity\": sensitivity,\n",
                "        \"specificity\": specificity,\n",
                "        \"ppv\": ppv,\n",
                "        \"npv\": npv,\n",
                "        \"accuracy\": accuracy,\n",
                "        \"sensitivity_at_95_recall\": sensitivity_at_95_recall,\n",
                "        \"threshold_at_95_recall\": threshold_at_95_recall,\n",
                "        \"confusion_matrix\": cm,\n",
                "        \"pr_curve\": (precisions, recalls, thresholds),\n",
                "    }\n",
                "\n",
                "\n",
                "metrics = compute_metrics(\n",
                "    df[\"y_true\"].values,\n",
                "    df[\"y_pred\"].values,\n",
                "    df[\"y_score\"].values\n",
                ")\n",
                "\n",
                "print(\"=\" * 50)\n",
                "print(\"EVALUATION METRICS\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"AUC-ROC:                    {metrics['auc_roc']:.3f}\")\n",
                "print(f\"Sensitivity (Recall):       {metrics['sensitivity']:.3f}\")\n",
                "print(f\"Specificity:                {metrics['specificity']:.3f}\")\n",
                "print(f\"PPV (Precision):            {metrics['ppv']:.3f}\")\n",
                "print(f\"NPV:                        {metrics['npv']:.3f}\")\n",
                "print(f\"Accuracy:                   {metrics['accuracy']:.3f}\")\n",
                "print(f\"Sensitivity @ 95% Recall:   {metrics['sensitivity_at_95_recall']:.3f}\")\n",
                "print(f\"Threshold @ 95% Recall:     {metrics['threshold_at_95_recall']:.3f}\")\n",
                "print(\"=\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Visualization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "# 1. Confusion Matrix\n",
                "ax1 = axes[0]\n",
                "sns.heatmap(\n",
                "    metrics[\"confusion_matrix\"],\n",
                "    annot=True,\n",
                "    fmt=\"d\",\n",
                "    cmap=\"Blues\",\n",
                "    xticklabels=[\"Non-Urgent\", \"Urgent\"],\n",
                "    yticklabels=[\"Non-Urgent\", \"Urgent\"],\n",
                "    ax=ax1\n",
                ")\n",
                "ax1.set_xlabel(\"Predicted\")\n",
                "ax1.set_ylabel(\"Actual\")\n",
                "ax1.set_title(\"Confusion Matrix\")\n",
                "\n",
                "# 2. Precision-Recall Curve\n",
                "ax2 = axes[1]\n",
                "precisions, recalls, _ = metrics[\"pr_curve\"]\n",
                "ax2.plot(recalls, precisions, 'b-', linewidth=2)\n",
                "ax2.axhline(y=metrics[\"sensitivity_at_95_recall\"], color='r', linestyle='--', label=f'Precision @ 95% Recall')\n",
                "ax2.axvline(x=0.95, color='g', linestyle='--', label='95% Recall')\n",
                "ax2.set_xlabel(\"Recall\")\n",
                "ax2.set_ylabel(\"Precision\")\n",
                "ax2.set_title(\"Precision-Recall Curve\")\n",
                "ax2.legend(loc='lower left')\n",
                "ax2.set_xlim([0, 1])\n",
                "ax2.set_ylim([0, 1])\n",
                "\n",
                "# 3. Score Distribution\n",
                "ax3 = axes[2]\n",
                "urgent_scores = df[df[\"y_true\"] == 1][\"y_score\"]\n",
                "non_urgent_scores = df[df[\"y_true\"] == 0][\"y_score\"]\n",
                "ax3.hist(non_urgent_scores, bins=20, alpha=0.5, label=\"Non-Urgent\", color=\"green\")\n",
                "ax3.hist(urgent_scores, bins=20, alpha=0.5, label=\"Urgent\", color=\"red\")\n",
                "ax3.axvline(x=0.5, color='black', linestyle='--', label='Default Threshold')\n",
                "ax3.set_xlabel(\"Confidence Score\")\n",
                "ax3.set_ylabel(\"Count\")\n",
                "ax3.set_title(\"Score Distribution by Class\")\n",
                "ax3.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig(EVAL_DIR / \"metrics_visualization.png\", dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(f\"Saved visualization to: {EVAL_DIR / 'metrics_visualization.png'}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Example Outputs with Provenance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Display 10 example outputs\n",
                "print(\"=\" * 70)\n",
                "print(\"EXAMPLE OUTPUTS (10 samples)\")\n",
                "print(\"=\" * 70)\n",
                "\n",
                "examples = df.sample(min(10, len(df)), random_state=42)\n",
                "\n",
                "for i, row in examples.iterrows():\n",
                "    correct = \"✓\" if row[\"y_true\"] == row[\"y_pred\"] else \"✗\"\n",
                "    print(f\"\\n[{correct}] ID: {row['id']}\")\n",
                "    print(f\"    Ground Truth: {row['urgency'].upper()}\")\n",
                "    print(f\"    Prediction:   {row['predicted_urgency'].upper()} (confidence: {row['confidence']:.2f})\")\n",
                "    print(f\"    Finding:      {row['primary_finding']}\")\n",
                "    print(f\"    Explanation:  {row['explanation'][:80]}...\" if len(row.get('explanation', '')) > 80 else f\"    Explanation:  {row.get('explanation', 'N/A')}\")\n",
                "\n",
                "print(\"\\n\" + \"=\" * 70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Clinician Rating Interface\n",
                "\n",
                "Simple CSV-based interface for human evaluation."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create clinician rating template\n",
                "rating_template = examples[[\"id\", \"urgency\", \"predicted_urgency\", \"confidence\"]].copy()\n",
                "rating_template[\"explanation\"] = examples[\"explanation\"].values\n",
                "rating_template[\"finding\"] = examples[\"primary_finding\"].values\n",
                "rating_template[\"clinician_rating\"] = \"\"  # To be filled: 1-5 or Accept/Reject\n",
                "rating_template[\"comments\"] = \"\"  # Optional comments\n",
                "\n",
                "# Save template\n",
                "rating_file = EVAL_DIR / \"clinician_rating_template.csv\"\n",
                "rating_template.to_csv(rating_file, index=False)\n",
                "print(f\"Clinician rating template saved to: {rating_file}\")\n",
                "print(\"\\nInstructions:\")\n",
                "print(\"1. Open the CSV file\")\n",
                "print(\"2. For each row, rate the explanation (1-5 or Accept/Reject)\")\n",
                "print(\"3. Add optional comments\")\n",
                "print(\"4. Save and re-run this notebook to include ratings in final report\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save Metrics Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create summary report\n",
                "report = {\n",
                "    \"model\": \"google/medgemma-4b-it\",\n",
                "    \"task\": \"CXR Urgency Classification\",\n",
                "    \"n_samples\": len(df),\n",
                "    \"metrics\": {\n",
                "        \"auc_roc\": float(metrics[\"auc_roc\"]),\n",
                "        \"sensitivity\": float(metrics[\"sensitivity\"]),\n",
                "        \"specificity\": float(metrics[\"specificity\"]),\n",
                "        \"ppv\": float(metrics[\"ppv\"]),\n",
                "        \"npv\": float(metrics[\"npv\"]),\n",
                "        \"accuracy\": float(metrics[\"accuracy\"]),\n",
                "        \"sensitivity_at_95_recall\": float(metrics[\"sensitivity_at_95_recall\"]),\n",
                "    },\n",
                "    \"confusion_matrix\": metrics[\"confusion_matrix\"].tolist(),\n",
                "}\n",
                "\n",
                "# Save JSON report\n",
                "report_file = EVAL_DIR / \"evaluation_report.json\"\n",
                "with open(report_file, \"w\") as f:\n",
                "    json.dump(report, f, indent=2)\n",
                "\n",
                "print(f\"Evaluation report saved to: {report_file}\")\n",
                "\n",
                "# Print summary\n",
                "print(\"\\n\" + \"=\" * 50)\n",
                "print(\"FINAL EVALUATION SUMMARY\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Samples evaluated: {report['n_samples']}\")\n",
                "print(f\"AUC-ROC: {report['metrics']['auc_roc']:.3f}\")\n",
                "print(f\"Sensitivity: {report['metrics']['sensitivity']:.3f}\")\n",
                "print(f\"PPV: {report['metrics']['ppv']:.3f}\")\n",
                "print(\"=\" * 50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Next Steps\n",
                "\n",
                "✓ Metrics computed and saved  \n",
                "✓ Visualization generated  \n",
                "✓ Clinician rating template created  \n",
                "\n",
                "**Proceed to:**\n",
                "1. Complete clinician ratings (manual step)\n",
                "2. Deploy demo app (`demo_app/`)\n",
                "3. Create video demonstration\n",
                "\n",
                "### ⚠️ Disclaimer\n",
                "These metrics are computed on a test dataset and may not reflect real-world clinical performance. Always validate with independent clinical studies before deployment."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}